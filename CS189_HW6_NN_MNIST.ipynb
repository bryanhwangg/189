{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanhwangg/189/blob/main/CS189_HW6_NN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxYASTXBDVc3"
      },
      "source": [
        "# CS 189 HW 6: Neural Networks\n",
        "**Note:** before starting this notebook, please make a copy of it, otherwise your changes will not persist.\n",
        "\n",
        "This part of the assignment is designed to get you familiar with how engineerings in the real world train neural network systems. It isn't designed to be difficult. In fact, everything you need to complete the assignment is available directly on the pytorch website [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html). This note book will have the following components:\n",
        "\n",
        "1. Understanding the basics of Pytorch (no deliverables)\n",
        "2. Training a simple neural network on MNIST (Deliverable = training graphs)\n",
        "3. Train a model on CIFAR-10 for Kaggle using transfer learning (Deliverable = kaggle submission and explanation of methods)\n",
        "\n",
        "The last part of this notebook is left open for you to explore as many techniques as you want to do as well as possible on the dataset.\n",
        "\n",
        "You will also get practice being an ML engineer by reading documentation and using it to implement models. The first section of this notebook will cover an outline of what you need to know -- we are confident that you can find the rest on your own.\n",
        "\n",
        "Note that like all other assignments, you are free to use this notebook or not. You just need to complete the deliverables and turn in your code. If you want to run everything outside of the notebook, make sure to appropriately install pytorch to download the datasets and copy out the code for kaggle submission. If you don't want to use pytorch and instead want to use Tensorflow, feel free, but you may still need to install pytorch to download the datasets. That said, we will recommend pytorch over tensorflow since the latter has a somewhat steep learning curve and the former is more accessible to beginners.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qoJVI0RCyaH"
      },
      "source": [
        "# Imports for pytorch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-GpmsVIEYLn"
      },
      "source": [
        "# 1. Understanding Pytorch\n",
        "\n",
        "Pytorch is based on the \"autograd\" paradigm. Essentially, you perform operations on multi-dimensional arrays like in numpy, except pytorch will automatically handle gradient tracking. In this section you will understand how to use pytorch.\n",
        "\n",
        "This section should help you understand the full pipeline of creating and training a model in pytorch. Feel free to re-use code from this section in the assigned tasks.\n",
        "\n",
        "Content in this section closely follows this pytorch tutorial: https://pytorch.org/tutorials/beginner/basics/intro.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgaXYlojE5Pm"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "Tensors can be created from numpy data or by using pytorch directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iKkuKmnFNvP"
      },
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "np_rand_array = rand_tensor.numpy()\n",
        "\n",
        "print(f\"Tensor from np: \\n {x_np} \\n\")\n",
        "print(f\"Rand Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Rand Numpy Array: \\n {np_rand_array} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEeXQUJqFk0k"
      },
      "source": [
        "They also support slicing and math operations very similar to numpy. See the examples below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIGbRiM-FfCx"
      },
      "source": [
        "# Slicing\n",
        "tensor = torch.ones(4, 4)\n",
        "print('First row: ',tensor[0])\n",
        "print('First column: ', tensor[:, 0])\n",
        "\n",
        "# Matrix Operations\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "# Getting a single item\n",
        "scalar = torch.sum(y1) # sums all elements\n",
        "item = scalar.item()\n",
        "print(\"Sum as a tensor:\", scalar, \", Sum as an item:\", item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0fscsOfH3dz"
      },
      "source": [
        "## Autograd\n",
        "This small section shows you how pytorch computes gradients. When we create tenors, we can set `requires_grad` to be true to indicate that we are using gradients. For most of the work that you actually do, you will use the `nn` package, which automatically sets all parameter tensors to have `requires_grad=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1btdxFJIvz_"
      },
      "source": [
        "# Below is an example of computing the gradient for a single data point in logistic regression using pytorch's autograd.\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(1) # label\n",
        "w = torch.randn(5, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "pred = torch.sigmoid(torch.matmul(x, w) + b)\n",
        "loss = torch.nn.functional.binary_cross_entropy(pred, y)\n",
        "loss.backward() # Computers gradients\n",
        "print(\"W gradient:\", w.grad)\n",
        "print(\"b gradient:\", b.grad)\n",
        "\n",
        "# when we want to actually take an update step, we can use optimizers:\n",
        "optimizer = torch.optim.SGD([w, b], lr=0.1)\n",
        "print(\"Weight before\", w)\n",
        "optimizer.step() # use the computed gradients to update\n",
        "# Print updated weights\n",
        "print(\"Updated weight\", w)\n",
        "\n",
        "# Performing operations with gradients enabled is slow...\n",
        "# You can disable gradient computation using the following enclosure:\n",
        "with torch.no_grad():\n",
        "    # Perform operations without gradients\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n9WqzZdGf-J"
      },
      "source": [
        "## Devices\n",
        "Pytorch supports accelerating computation using GPUs which are available on google colab. To use a GPU on google colab, go to runtime -> change runtime type -> select GPU.\n",
        "\n",
        "Note that there is some level of strategy for knowing when to use which runtime type. Colab will kick users off of GPU for a certain period of time if you use it too much. Thus, its best to run simple models and prototype to get everything working on CPU, then switch the instance type over to GPU for training runs and parameter tuning.\n",
        "\n",
        "Its best practice to make sure your code works on any device (GPU or CPU) for pytorch, but note that numpy operations can only run on the CPU. Here is a standard flow for using GPU acceleration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwHYcdpmG0Tq"
      },
      "source": [
        "# Determine the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", device)\n",
        "# Next create your tensors\n",
        "tensor = torch.zeros(4, 4, requires_grad=True)\n",
        "# Move the tensor to the device you want to use\n",
        "tensor = tensor.to(device)\n",
        "\n",
        "# Perform whatever operations you want.... (often this will involve gradients)\n",
        "# These operations will be accelerated by GPU.\n",
        "tensor = 10*(tensor + 1)\n",
        "\n",
        "# bring the tensor back to CPU, first detaching it from any gradient computations\n",
        "tensor = tensor.detach().cpu()\n",
        "\n",
        "tensor_np = tensor.numpy() # Convert to numpy if you want to perform numpy operations."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZvRtdC1NZFe"
      },
      "source": [
        "## The NN Package\n",
        "Pytorch implements composable blocks in `Module` classes. All layers and modules in pytorch inherit from `nn.Module`. When you make a module you need to implement two functions: `__init__(self, *args, **kwargs)` and `foward(self, *args, **kwargs)`. Modules also have some nice helper functions, namely `parameters` which will recursively return all of the parameters. Here is an example of a logistic regression model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNvfGFz4OTp_"
      },
      "source": [
        "class Perceptron(nn.Module):\n",
        "  def __init__(self, in_dim):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Linear(in_dim, 1) # This is a linear layer, it computes Xw + b\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.sigmoid(self.layer(x)).squeeze(-1)\n",
        "\n",
        "perceptron = Perceptron(10)\n",
        "perceptron = perceptron.to(device) # Move all the perceptron's tensors to the device\n",
        "print(\"Parameters\", list(perceptron.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHV9D362PE0w"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Pytorch has nice interfaces for using datasets. Suppose we create a logistic regression dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7VBsu23Pj9w"
      },
      "source": [
        "c1_x1, c1_x2 = np.random.multivariate_normal([-2.5,3], [[1, 0.3],[0.3, 1]], 100).T\n",
        "c2_x1, c2_x2 = np.random.multivariate_normal([1,1], [[2, 1],[1, 2]], 100).T\n",
        "c1_X = np.vstack((c1_x1, c1_x2)).T\n",
        "c2_X = np.vstack((c2_x1, c2_x2)).T\n",
        "train_X = np.concatenate((c1_X, c2_X))\n",
        "train_y = np.concatenate((np.zeros(100), np.ones(100)))\n",
        "# Shuffle the data\n",
        "permutation = np.random.permutation(train_X.shape[0])\n",
        "train_X = train_X[permutation, :]\n",
        "train_y = train_y[permutation]\n",
        "# Plot the data\n",
        "plt.plot(c1_x1, c1_x2, 'x')\n",
        "plt.plot(c2_x1, c2_x2, 'o')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ0_Dch3P4Cd"
      },
      "source": [
        "We can then create a pytorch dataset object as follows. Often times, the default pytorch datasets will create these objects for you. Then, we can apply dataloaders to iterate over the dataset in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFUJJdgdPsht"
      },
      "source": [
        "dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "# We can create a dataloader that iterates over the dataset in batches.\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "for x, y in dataloader:\n",
        "    print(\"Batch x:\", x)\n",
        "    print(\"Batch y:\", y)\n",
        "    break\n",
        "\n",
        "# Clean up the dataloader as we make a new one later\n",
        "del dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vaq5wwPQtNO"
      },
      "source": [
        "## Training Loop Example\n",
        "Here is an example of training a full logistic regression model in pytorch. Note the extensive use of modules -- modules can be used for storing networks, computation steps etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WwjYScvQms3"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", device)\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "num_features = dataset[0][0].shape[0]\n",
        "model = Perceptron(num_features).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.BCELoss()\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model.train() # Put model in training mode\n",
        "for epoch in range(epochs):\n",
        "    training_losses = []\n",
        "    for x, y in tqdm.notebook.tqdm(dataloader, unit=\"batch\"):\n",
        "        x, y = x.float().to(device), y.float().to(device)\n",
        "        optimizer.zero_grad() # Remove the gradients from the previous step\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_losses.append(loss.item())\n",
        "    print(\"Finished Epoch\", epoch + 1, \", training loss:\", np.mean(training_losses))\n",
        "\n",
        "# We can run predictions on the data to determine the final accuracy.\n",
        "with torch.no_grad():\n",
        "    model.eval() # Put model in eval mode\n",
        "    num_correct = 0\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.float().to(device), y.float().to(device)\n",
        "        pred = model(x)\n",
        "        num_correct += torch.sum(torch.round(pred) == y).item()\n",
        "    print(\"Final Accuracy:\", num_correct / len(dataset))\n",
        "    model.train() # Put model back in train mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOxM8vT4SsD9"
      },
      "source": [
        "# Task 1: CNN For FashionMNIST\n",
        "Earlier in this course you trained SVMs and GDA models on MNIST. Now you will train a Convolutional Neural Network (CNN) on an MNIST-like dataset. Your deliverables are as follows:\n",
        "\n",
        "1. Code for training a CNN on MNIST (can be in code appendix, tagged in your submission).\n",
        "2. A plot of the training loss and validation loss for each epoch of training after trainnig for at least 8 epochs.\n",
        "3. A plot of the training and validation accuracy, showing that it is at least 82% for validation by the end of training.\n",
        "\n",
        "Below we will create the training and validation datasets for you, and provide a very basic skeleton of the code. Please leverage the example training loop from above.\n",
        "\n",
        "Here are some of the components you should consider using:\n",
        "1. `nn.Conv2d`\n",
        "2. `nn.ReLU`\n",
        "3. `nn.Linear`\n",
        "3. `nn.CrossEntropyLoss`: if you choose to use `nn.CrossEntropyLoss` or `F.cross_entropy`, DO NOT add an explicit softmax layer in your neural network. PyTorch devs found it more numerically stable to combine softmax and cross entropy loss into a single module and if you explicitly attach a softmax layer at the end of your model, you would unintentionally be applying it twice, which can degrade performance.\n",
        "5. `nn.MaxPooling2d` (though many implementations without it exist; for example, you can also do strided convolutions instead of a pooling layer!)\n",
        "\n",
        "Here are challenges you will need to overcome:\n",
        "1. You need to write code for plotting.\n",
        "2. You need to find appropriate hyper-parameters to achieve good accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sSW4B4yR-G2"
      },
      "source": [
        "# Creating the datasets\n",
        "transform = torchvision.transforms.ToTensor() # feel free to modify this as you see fit.\n",
        "\n",
        "training_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "validation_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training a neural network, let's visualize our data first! Running the cell below will display the first 9 images in a 3 by 3 grid."
      ],
      "metadata": {
        "id": "JTXLVZJT1xda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = [training_data[i][0] for i in range(9)]\n",
        "plt.imshow(torchvision.utils.make_grid(torch.stack(images), nrow=3, padding=5).numpy().transpose((1, 2, 0)))"
      ],
      "metadata": {
        "id": "lu1VXUH51t8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4PPZnVCXH8_",
        "outputId": "09c9995c-b213-48be-be9d-ddb7ab802e30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "from torch.utils.data import DataLoader\n",
        "torch.manual_seed(189)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=1)  # output is 32, 28, 28\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)  # output is 32, 14, 14\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1)  # output is 64, 14, 14\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)  # output is 64, 7, 7\n",
        "\n",
        "        self.fc = nn.Linear(64 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = x.view(-1, 64 * 7 * 7) # flatten\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "model = CNN()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "\n",
        "num_epochs = 8\n",
        "losses = []\n",
        "\n",
        "# Training loop with loss and accuracy printing per epoch\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over batches in the dataloader\n",
        "    for X, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = total_loss / len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8, Loss: 0.4168, Accuracy: 86.48%\n",
            "Epoch 2/8, Loss: 0.2627, Accuracy: 90.62%\n",
            "Epoch 3/8, Loss: 0.2320, Accuracy: 91.78%\n",
            "Epoch 4/8, Loss: 0.2124, Accuracy: 92.34%\n",
            "Epoch 5/8, Loss: 0.1959, Accuracy: 92.99%\n",
            "Epoch 6/8, Loss: 0.1827, Accuracy: 93.50%\n",
            "Epoch 7/8, Loss: 0.1708, Accuracy: 93.72%\n",
            "Epoch 8/8, Loss: 0.1617, Accuracy: 94.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofNTv8Z57x1w"
      },
      "source": [
        "After finishing this section, please go to the next notebook to finish the second part."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1,9), losses)"
      ],
      "metadata": {
        "id": "B2tYr1001xq9",
        "outputId": "b1503d0a-8d66-4285-8b8f-bbc8c64df5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (8,) and (15000,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-319dd3df25df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m ) -> list[Line2D]:\n\u001b[0;32m-> 3829\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3830\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m         \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \"\"\"\n\u001b[1;32m   1776\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    495\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (8,) and (15000,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmsQpx-L3zVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}